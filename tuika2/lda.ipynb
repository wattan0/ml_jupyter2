{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピックモデル（LDA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9つの文書 (11単語)\n",
    "docs = [\"dog cute cat cute\",\n",
    "            \"dog cat cute\",\n",
    "            \"cute dog cute cat\",\n",
    "            \"soccer team fan\",\n",
    "            \"baseball team soccer team\",\n",
    "            \"baseball fan soccer fan\",\n",
    "            \"Japan China U.S.\",\n",
    "            \"China large U.S. large\",\n",
    "            \"China large Japan\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前処理（Bag of Words化）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書を各単語の集合にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'dog',\n",
       "  1: 'cute',\n",
       "  2: 'cat',\n",
       "  3: 'soccer',\n",
       "  4: 'team',\n",
       "  5: 'fan',\n",
       "  6: 'baseball',\n",
       "  7: 'Japan',\n",
       "  8: 'China',\n",
       "  9: 'U.S.',\n",
       "  10: 'large'},\n",
       " [[0, 1, 2, 1],\n",
       "  [0, 2, 1],\n",
       "  [1, 0, 1, 2],\n",
       "  [3, 4, 5],\n",
       "  [6, 4, 3, 4],\n",
       "  [6, 5, 3, 5],\n",
       "  [7, 8, 9],\n",
       "  [8, 10, 9, 10],\n",
       "  [8, 10, 7]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文書を単語のリストにする\n",
    "_docs = []\n",
    "for doc in docs:\n",
    "    _docs.append(doc.split())\n",
    "\n",
    "words = []\n",
    "dic = {}\n",
    "bw = []\n",
    "for _doc in _docs:\n",
    "    ws = []\n",
    "    for word in _doc:\n",
    "        if word in words:\n",
    "            ws.append(words.index(word))\n",
    "        else:\n",
    "            words.append(word)\n",
    "            id_ = words.index(word)\n",
    "            ws.append(id_)\n",
    "            dic[id_] = word\n",
    "    bw.append(ws)\n",
    "\n",
    "dic, bw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation(LDA)のモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boldsymbol{\\theta}_{i}$を文書$i$のトピック分布、$\\boldsymbol{\\phi}_{k}$をトピック$k$の単語分布、$z_{ij}$を文書$i$中の単語$j$に割り当てられるトピック、  \n",
    "$w_{ij}$を文書$i$中の単語$j$、$D$を文書数、$K$をトピック数、$N$を単語数、$\\alpha, \\beta$をディリクレ分布のハイパーパラメータとすると、  \n",
    "文書の生成過程は以下のとおりである。\n",
    "\n",
    "1. $\\boldsymbol{\\theta}_{i} \\sim \\text{Dir}(\\alpha) \\qquad (i=1, 2, ..., D)$\n",
    "2. $\\boldsymbol{\\phi}_{k} \\sim \\text{Dir}(\\beta) \\qquad (K=1, 2, ..., K)$\n",
    "3. 各文書$i \\ (i=1, 2, ..., D)$と単語$j \\ (j=1, 2, ..., N_{i})$に対して、  \n",
    "     a) $z_{ij} \\sim \\text{Multinomial}(\\boldsymbol{\\theta}_{i})$  \n",
    "     b) $w_{ij} \\sim \\text{Multinomial}(\\boldsymbol{\\phi}_{z_{ij}})$\n",
    "     \n",
    "ただし、$\\text{Dir}(\\cdot)$はディリクレ分布、$\\text{Multinomial}(\\cdot)$は多項分布である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックモデルではある単語$w$が与えられたときのトピック$z$の事後確率$p(z | w)$を知る必要がある。\n",
    "\n",
    "しかし、これは解析的に解くことができないので、今回は崩壊型ギブスサンプリングを用いて事後分布を推定する。\n",
    "\n",
    "崩壊型ギブスサンプリングとは、確率モデルから一部の確率変数を周辺化除去することでサンプリング効率を高めたギブスサンプリング（ある確率変数をサンプリングする際、その他の確率変数を固定した確率分布からサンプリングする手法）である。\n",
    "\n",
    "文書全体$W$が与えられたとき、同時分布は以下のようになる。\n",
    "\n",
    "$p(Z, W, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}; \\alpha, \\beta) = \\prod_{k=1}^{K} p(\\boldsymbol{\\phi}_{i}; \\beta) \\prod_{i=1}^{D} p(\\boldsymbol{\\theta}_{i}; \\alpha) \\prod_{j=1}^{N_{i}} p(Z_{ij} | \\boldsymbol{\\theta}_{i}) p(W_{ij} | \\boldsymbol{\\phi}_{z_{ij}})$\n",
    "\n",
    "計算の詳細は省くが、パラメータ$\\boldsymbol{\\theta}, \\boldsymbol{\\phi}$を周辺化除去することで、$Z_{ij}$の事後分布は以下に従う。\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(Z_{ij}=k | Z_{-(ij)}, W; \\alpha, \\beta) &\\propto& p(Z_{ij}=k, Z_{-(ij)}, W; \\alpha, \\beta) \\\\\n",
    "&\\propto& (N_{ik(\\cdot)}^{-(ij)} + \\alpha) \\frac{N_{(\\cdot)kj}^{-(ij)} + \\beta}{\\sum_{r=1}^{N_{i}} (N_{(\\cdot)kr}^{-(ij)} + \\beta)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "ただし、$Z_{-(ij)}$は$Z$から$Z_{ij}$を除いたものである。また$N_{ik(\\cdot)}^{-(ij)}$は文書$i$中の単語$j$を除いた上での文書$i$中のトピック$k$の単語数、$N_{(\\cdot)kj}^{-(ij)}$は文書$i$中の単語$j$を除いた上での全文書中でのトピック$k$の単語$j$の数を表す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_words = len(words)\n",
    "n_docs = len(bw)\n",
    "n_topics = 3\n",
    "\n",
    "# Zの初期値は一様サンプリング\n",
    "Z = [[np.random.randint(n_topics) for _ in bw[i]] for i in range(n_docs)]\n",
    "\n",
    "# 文書i内で、トピックkに割り当てられた単語数\n",
    "Nik = np.zeros((n_docs, n_topics), dtype=int)\n",
    "# トピックkに割り当てられた単語jの数\n",
    "Nkj = np.zeros((n_topics, n_words), dtype=int)\n",
    "for i in range(n_docs):\n",
    "    for w, z in zip(bw[i], Z[i]):\n",
    "        Nik[i, z] += 1\n",
    "        Nkj[z, w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.01\n",
    "\n",
    "def conditional_distribution(i, j):\n",
    "    # p(z=k | Z, W)\n",
    "    left = Nik[i] + alpha\n",
    "    right = (Nkj[:, j] + beta) / (Nkj.sum(axis=1) + beta*n_words)\n",
    "    \n",
    "    p_z = left * right\n",
    "    return p_z / np.sum(p_z)\n",
    "\n",
    "def sample_z(p):\n",
    "    return np.random.choice(n_topics, p=p)\n",
    "\n",
    "max_iter = 10\n",
    "for _ in range(max_iter):\n",
    "    for i in range(n_docs):\n",
    "        for j, (w, z) in enumerate(zip(bw[i], Z[i])):\n",
    "            # 該当の単語を抜く\n",
    "            Nik[i, z] -= 1\n",
    "            Nkj[z, w] -= 1\n",
    "\n",
    "            p_z = conditional_distribution(i, w)\n",
    "            z_new = sample_z(p_z)\n",
    "            \n",
    "            # 更新\n",
    "            Z[i][j]= z_new\n",
    "            Nik[i, z_new] += 1\n",
    "            Nkj[z_new, w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事後分布$p(Z | W)$からのサンプリング結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0],\n",
       " [0, 0, 0],\n",
       " [0, 0, 0, 0],\n",
       " [2, 2, 2],\n",
       " [2, 2, 2, 2],\n",
       " [2, 2, 2, 2],\n",
       " [1, 1, 1],\n",
       " [1, 1, 1, 1],\n",
       " [1, 1, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
